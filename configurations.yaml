---

# Paths
main_path: "/media/ulgen/Samsung/contradiction_data/"
nli_set_train: "/media/ulgen/Samsung/contradiction_data/nli_sets/train.jsonl"
nli_set_dev: "/media/ulgen/Samsung/contradiction_data/nli_sets/dev.jsonl"
nli_set_test: "/media/ulgen/Samsung/contradiction_data/nli_sets/test.jsonl"
processed_nli: "/media/ulgen/Samsung/contradiction_data/processed_nli/"
results: "/media/ulgen/Samsung/contradiction_data/results/"


# Trained model paths
esim: "/media/ulgen/Samsung/contradiction_data/nli_models/esim/"
decomposable_attention: "/media/ulgen/Samsung/contradiction_data/nli_models/decomposable_attention/"

# Transformers Paths
fasttext: "/media/ulgen/Samsung/contradiction_data/transformers/fasttext/"
word2vec: "/media/ulgen/Samsung/contradiction_data/transformers/word2vec/"
glove: "/media/ulgen/Samsung/contradiction_data/transformers/glove/"
ontonotes5: "/media/ulgen/Samsung/contradiction_data/transformers/ontonotes5/"
bert_embeddings: "/media/ulgen/Samsung/contradiction_data/transformers/bert_embeddings/"
bert_tf_hub_contextualized: "/usr/local/bert_tf_hub"

# Network Parameters
nr_unk: 100
max_length: 64
nr_hidden: 512
learn_rate: 0.0002
batch_size: 32
nr_epoch: 4
nr_class: 3
early_stopping: 2


#Decomposable attention with Bert Actual embeddings:
# batch_size -> 1024
# hidden_size -> 512
# learning_rate -> 0.0004

#Decomposable attention with pre-trained:
# batch_size -> 1024
# hidden_size -> 200
# learning_rate -> 0.001


# Esim with Bert Based:
# batch_size -> 32
# hidden_size -> 512
# learning_rate -> 0.0002

# Esim with pre trained:
# batch_size -> 32
# hidden_size -> 300
# learning_rate -> 0.0004
