{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7b645a",
   "metadata": {},
   "source": [
    "# SNLI with spaCy & ESIM\n",
    "Load the SNLI dataset, preprocess with spaCy, build embeddings, pad sequences, and prepare inputs for the ESIM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9063b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from models.esim import ESIM\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils import load_nli_data\n",
    "\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_lg\",\n",
    "    exclude=[\n",
    "        \"parser\",\n",
    "        \"tagger\",\n",
    "        \"ner\",\n",
    "        \"textcat\",\n",
    "        \"lemmatizer\",\n",
    "        \"attribute_ruler\",\n",
    "        \"tok2vec\",\n",
    "    ],\n",
    ")\n",
    "print(\"unique vector size\", len(nlp.vocab.vectors))\n",
    "\n",
    "# Hyper‑parameters\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 3\n",
    "HIDDEN = 512\n",
    "NUM_CLASSES = 3\n",
    "LR = 1e-3\n",
    "NR_UNK = 100\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add47281",
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train = load_nli_data(\"data/snli_1.0_train.jsonl\")\n",
    "snli_dev = load_nli_data(\"data/snli_1.0_dev.jsonl\")\n",
    "snli_test = load_nli_data(\"data/snli_1.0_test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c810e8",
   "metadata": {},
   "source": [
    "# 1-Tokenization and Preprocessing NLI Pairs\n",
    "* Following function tokenizes the premise and hypothesis sentences using spaCy. Gets their token ids from NLP object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3563aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_to_ids(texts, max_length=64, nr_unk=100):\n",
    "    \"\"\"\n",
    "    Convert texts to token IDs using spaCy vocabulary.\n",
    "    0=PAD, 1..nr_unk=OOV buckets, nr_unk+1..=vector tokens.\n",
    "    \"\"\"\n",
    "    vec_key2row = nlp.vocab.vectors.key2row  # dict: lex_id -> row_index\n",
    "    all_ids = np.zeros((len(texts), max_length), dtype=np.int32)\n",
    "\n",
    "    for i, doc in enumerate(tqdm(nlp.pipe(texts, n_process=-1), total=len(texts), desc=\"Tokenizing\")):\n",
    "        seq_ids = []\n",
    "        for token in doc[:max_length]:\n",
    "            row = vec_key2row.get(token.orth)\n",
    "            if row is not None and token.vector_norm > 0:\n",
    "                seq_ids.append(row + nr_unk + 1)\n",
    "            else:\n",
    "                seq_ids.append(1 + (hash(token.text) % nr_unk))\n",
    "\n",
    "        # pad/truncate\n",
    "        arr = np.zeros(max_length, dtype=np.int32)\n",
    "        arr[: len(seq_ids)] = seq_ids\n",
    "        all_ids[i] = arr\n",
    "\n",
    "    return all_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832bda98",
   "metadata": {},
   "source": [
    "* Following function uses \"tokenize_to_ids\" to convert the sentences into token ids. Process is repeated for all NLI sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f10687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_nli_data(df, name, max_length=MAX_LEN, nr_unk=NR_UNK):\n",
    "\n",
    "    # Save the processed data\n",
    "    output_path = f\"data/{name}.npz\"\n",
    "\n",
    "    np.savez_compressed(\n",
    "        output_path,\n",
    "        sentence1_tokens=tokenize_to_ids(\n",
    "            df[\"sentence1\"], max_length=max_length, nr_unk=nr_unk\n",
    "        ),\n",
    "        sentence2_tokens=tokenize_to_ids(\n",
    "            df[\"sentence2\"], max_length=max_length, nr_unk=nr_unk\n",
    "        ),\n",
    "        label=df[\"label\"],\n",
    "    )\n",
    "\n",
    "    print(f\"Saved {output_path}\")\n",
    "\n",
    "\n",
    "# Process each dataset\n",
    "process_and_save_nli_data(snli_train, \"train\")\n",
    "process_and_save_nli_data(snli_dev, \"dev\")\n",
    "process_and_save_nli_data(snli_test, \"test\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44c934d8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "data = np.load(\"data/train.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f15d9d",
   "metadata": {},
   "source": [
    "# 2-Extract Embedding Matrix from Spacy NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_spacy3(nlp, nr_unk=100):\n",
    "    \"\"\"\n",
    "    Build (PAD + OOV + spaCy) matrix for ESIM.\n",
    "      PAD row = 0\n",
    "      next nr_unk rows = random OOV\n",
    "      then spaCy.vectors rows\n",
    "    \"\"\"\n",
    "    vecs = nlp.vocab.vectors\n",
    "    n_rows, dim = vecs.shape\n",
    "    total = 1 + nr_unk + n_rows\n",
    "\n",
    "    # init\n",
    "    emb = np.zeros((total, dim), dtype=\"float32\")\n",
    "\n",
    "    # random OOV vectors (unit‑norm)\n",
    "    oov = np.random.normal(size=(nr_unk, dim)).astype(\"float32\")\n",
    "    oov /= np.linalg.norm(oov, axis=1, keepdims=True)\n",
    "    emb[1: nr_unk+1] = oov\n",
    "\n",
    "    # copy spaCy vectors\n",
    "    for lex_id, row in vecs.key2row.items():\n",
    "        emb[nr_unk + 1 + row] = vecs.data[row]\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_embeddings_spacy3(nlp, NR_UNK)\n",
    "np.save(\"data/embedding_matrix.npy\", embedding_matrix)\n",
    "print(\"Saved emb_matrix.npy with shape\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1701f",
   "metadata": {},
   "source": [
    "# 3-Train the ESIM Model\n",
    "* Load the Preprocessed data and the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0259eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data = np.load(\"data/train.npz\")\n",
    "dev_data = np.load(\"data/dev.npz\")\n",
    "test = np.load(\"data/test.npz\")\n",
    "\n",
    "x1_train = torch.tensor(train_data[\"sentence1_tokens\"], dtype=torch.long)\n",
    "x2_train = torch.tensor(train_data[\"sentence2_tokens\"], dtype=torch.long)\n",
    "y_train = torch.tensor(train_data[\"label\"], dtype=torch.long)\n",
    "\n",
    "x1_dev = torch.tensor(dev_data[\"sentence1_tokens\"], dtype=torch.long)\n",
    "x2_dev = torch.tensor(dev_data[\"sentence2_tokens\"], dtype=torch.long)\n",
    "y_dev = torch.tensor(dev_data[\"label\"], dtype=torch.long)\n",
    "\n",
    "x1_test = torch.tensor(test[\"sentence1_tokens\"], dtype=torch.long)\n",
    "x2_test = torch.tensor(test[\"sentence2_tokens\"], dtype=torch.long)\n",
    "y_test = torch.tensor(test[\"label\"], dtype=torch.long)\n",
    "\n",
    "# Datasets & loaders\n",
    "train_ds = TensorDataset(x1_train, x2_train, y_train)\n",
    "dev_ds = TensorDataset(x1_dev, x2_dev, y_dev)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(TensorDataset(x1_test, x2_test, y_test), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load embedding matrix\n",
    "emb_mat = torch.tensor(np.load(\"data/embedding_matrix.npy\"), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c947f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, optimizer, scaler, loss\n",
    "model = ESIM(\n",
    "    embedding_matrix=emb_mat,\n",
    "    hidden_size=HIDDEN,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=0.5,\n",
    "    padding_idx=0,\n",
    ").to(device)\n",
    "\n",
    "model = torch.compile(model, backend=\"inductor\") \n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=LR)\n",
    "scaler = GradScaler()\n",
    "crit = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lengths(x):\n",
    "    # count non-zero tokens per row\n",
    "    return (x != 0).sum(dim=1)\n",
    "\n",
    "\n",
    "def evaluate(loader, return_loss=False):\n",
    "    \"\"\"\n",
    "    Evaluate model on a DataLoader.\n",
    "    If return_loss=True, returns (avg_loss, accuracy), otherwise just accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, y in loader:\n",
    "            x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "            l1, l2 = compute_lengths(x1), compute_lengths(x2)\n",
    "            logits = model(x1, l1, x2, l2)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total_samples += y.size(0)\n",
    "            if return_loss:\n",
    "                loss = crit(logits, y)\n",
    "                total_loss += loss.item() * y.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    if return_loss:\n",
    "        return total_loss / total_samples, accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n",
    "    running_loss = 0\n",
    "    running_correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    for x1, x2, y in pbar:\n",
    "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "        l1, l2 = compute_lengths(x1), compute_lengths(x2)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        with autocast(device_type=device.type):\n",
    "            logits = model(x1, l1, x2, l2)\n",
    "            loss   = crit(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt); scaler.update()\n",
    "\n",
    "        bs = y.size(0)\n",
    "        running_loss    += loss.item() * bs\n",
    "        running_correct += (logits.argmax(1)==y).sum().item()\n",
    "        samples         += bs\n",
    "\n",
    "        avg_loss = running_loss / samples\n",
    "        avg_acc  = running_correct / samples\n",
    "        pbar.set_postfix(loss=f\"{avg_loss:.4f}\", acc=f\"{avg_acc:.4f}\")\n",
    "\n",
    "    # optional end‑of‑epoch eval\n",
    "    dev_loss, dev_acc = evaluate(dev_loader, return_loss=True)\n",
    "    print(f\"→ Dev  loss: {dev_loss:.4f}, acc: {dev_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"data/esim_nli_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fbcce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"data/esim_nli_model.pt\", map_location=device, weights_only=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bbde24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(test_loader, return_loss=True)\n",
    "print(f\"Test  loss: {test_loss:.4f},  acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c9106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contradiction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
