{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7b645a",
   "metadata": {},
   "source": [
    "# Spacy - Natural Language Inference (SNLI) Task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9063b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from models.esim import ESIM\n",
    "\n",
    "from utils.utils import (\n",
    "    load_nli_data,\n",
    "    get_embeddings_spacy,\n",
    "    tokenize_to_ids,\n",
    "    compute_lengths,\n",
    "    evaluate,\n",
    ")\n",
    "\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_lg\",\n",
    "    exclude=[\n",
    "        \"parser\",\n",
    "        \"tagger\",\n",
    "        \"ner\",\n",
    "        \"textcat\",\n",
    "        \"lemmatizer\",\n",
    "        \"attribute_ruler\",\n",
    "        \"tok2vec\",\n",
    "    ],\n",
    ")\n",
    "print(\"unique vector size\", len(nlp.vocab.vectors))\n",
    "\n",
    "# Hyper‑parameters\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 3\n",
    "HIDDEN = 512\n",
    "NUM_CLASSES = 3\n",
    "LR = 1e-3\n",
    "NR_UNK = 100\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "label_map = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2}\n",
    "\n",
    "# reverse it: id→name\n",
    "id2label = {v: k for k, v in label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add47281",
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train = load_nli_data(\"data/snli_1.0_train.jsonl\")\n",
    "snli_dev = load_nli_data(\"data/snli_1.0_dev.jsonl\")\n",
    "snli_test = load_nli_data(\"data/snli_1.0_test.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c810e8",
   "metadata": {},
   "source": [
    "# 1-Tokenization and Preprocessing NLI Pairs\n",
    "* * Following function uses \"tokenize_to_ids\" to convert the sentences into token ids. Process is repeated for all NLI sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f10687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_nli_data(df, name, nlp, max_length=MAX_LEN, nr_unk=NR_UNK):\n",
    "\n",
    "    np.savez_compressed(\n",
    "        f\"data/{name}.npz\",\n",
    "        sentence1_tokens=tokenize_to_ids(\n",
    "            df[\"sentence1\"], nlp=nlp, max_length=max_length, nr_unk=nr_unk\n",
    "        ),\n",
    "        sentence2_tokens=tokenize_to_ids(\n",
    "            df[\"sentence2\"], nlp=nlp, max_length=max_length, nr_unk=nr_unk\n",
    "        ),\n",
    "        label=df[\"label\"],\n",
    "    )\n",
    "\n",
    "    print(f\"Saved {name}\")\n",
    "\n",
    "\n",
    "# Process each dataset\n",
    "process_and_save_nli_data(snli_train, \"train\", nlp)\n",
    "process_and_save_nli_data(snli_dev, \"dev\", nlp)\n",
    "process_and_save_nli_data(snli_test, \"test\", nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f15d9d",
   "metadata": {},
   "source": [
    "# 2-Extract Embedding Matrix from Spacy NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = get_embeddings_spacy(nlp, NR_UNK)\n",
    "np.save(\"data/embedding_matrix.npy\", embedding_matrix)\n",
    "print(\"Saved emb_matrix.npy with shape\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739dac6",
   "metadata": {},
   "source": [
    "# 3- Load the Processed NLI Dataset and Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0259eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data = np.load(\"data/train.npz\")\n",
    "dev_data = np.load(\"data/dev.npz\")\n",
    "test = np.load(\"data/test.npz\")\n",
    "\n",
    "x1_train = torch.tensor(train_data[\"sentence1_tokens\"], dtype=torch.long)\n",
    "x2_train = torch.tensor(train_data[\"sentence2_tokens\"], dtype=torch.long)\n",
    "y_train = torch.tensor(train_data[\"label\"], dtype=torch.long)\n",
    "\n",
    "x1_dev = torch.tensor(dev_data[\"sentence1_tokens\"], dtype=torch.long)\n",
    "x2_dev = torch.tensor(dev_data[\"sentence2_tokens\"], dtype=torch.long)\n",
    "y_dev = torch.tensor(dev_data[\"label\"], dtype=torch.long)\n",
    "\n",
    "x1_test = torch.tensor(test[\"sentence1_tokens\"], dtype=torch.long)\n",
    "x2_test = torch.tensor(test[\"sentence2_tokens\"], dtype=torch.long)\n",
    "y_test = torch.tensor(test[\"label\"], dtype=torch.long)\n",
    "\n",
    "# Datasets & loaders\n",
    "train_ds = TensorDataset(x1_train, x2_train, y_train)\n",
    "dev_ds = TensorDataset(x1_dev, x2_dev, y_dev)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(TensorDataset(x1_test, x2_test, y_test), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load embedding matrix\n",
    "emb_mat = torch.tensor(np.load(\"data/embedding_matrix.npy\"), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1701f",
   "metadata": {},
   "source": [
    "# 4-Train the ESIM Model\n",
    "* Inıtialize the ESIM model with the embedding matrix and compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c947f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, optimizer, scaler, loss\n",
    "model = ESIM(\n",
    "    embedding_matrix=emb_mat,\n",
    "    hidden_size=HIDDEN,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout=0.5,\n",
    "    padding_idx=0,\n",
    ").to(device)\n",
    "\n",
    "model = torch.compile(model, backend=\"inductor\")\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=LR)\n",
    "scaler = GradScaler()\n",
    "crit = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1765849c",
   "metadata": {},
   "source": [
    "* Train the ESIM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n",
    "    running_loss = 0\n",
    "    running_correct = 0\n",
    "    samples = 0\n",
    "\n",
    "    for x1, x2, y in pbar:\n",
    "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "        l1, l2 = compute_lengths(x1), compute_lengths(x2)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        with autocast(device_type=device.type):\n",
    "            logits = model(x1, l1, x2, l2)\n",
    "            loss = crit(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "\n",
    "        bs = y.size(0)\n",
    "        running_loss += loss.item() * bs\n",
    "        running_correct += (logits.argmax(1) == y).sum().item()\n",
    "        samples += bs\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            loss=f\"{running_loss / samples:.4f}\", acc=f\"{running_correct / samples:.4f}\"\n",
    "        )\n",
    "\n",
    "    # optional end‑of‑epoch eval\n",
    "    dev_loss, dev_acc = evaluate(model, dev_loader, crit, device, return_loss=True)\n",
    "    print(f\"→ Dev  loss: {dev_loss:.4f}, acc: {dev_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c48c42",
   "metadata": {},
   "source": [
    "* Save the model and later load to see outsample test scores of model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"data/esim_nli_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fbcce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"data/esim_nli_model.pt\", map_location=device, weights_only=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bbde24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and test it on outsample dataset\n",
    "\n",
    "test_acc = evaluate(\n",
    "    model=model, loader=test_loader, crit=None, device=device, return_loss=False\n",
    ")\n",
    "print(f\"Test acc: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contradiction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
