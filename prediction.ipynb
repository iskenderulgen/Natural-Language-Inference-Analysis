{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db4bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from utils import (\n",
    "    tokenize_to_ids,\n",
    ")\n",
    "\n",
    "nlp = spacy.load(\n",
    "    \"en_core_web_lg\",\n",
    "    exclude=[\n",
    "        \"parser\",\n",
    "        \"tagger\",\n",
    "        \"ner\",\n",
    "        \"textcat\",\n",
    "        \"lemmatizer\",\n",
    "        \"attribute_ruler\",\n",
    "        \"tok2vec\",\n",
    "    ],\n",
    ")\n",
    "print(\"unique vector size\", len(nlp.vocab.vectors))\n",
    "\n",
    "# Hyper‑parameters\n",
    "MAX_LEN = 64\n",
    "NUM_CLASSES = 3\n",
    "NR_UNK = 100\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "label_map = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2}\n",
    "\n",
    "# reverse it: id→name\n",
    "id2label = {v: k for k, v in label_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4e24d4",
   "metadata": {},
   "source": [
    "# ESIM Model NLI Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf57f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_and_plot(model, text1: str, text2: str):\n",
    "    \"\"\"Run inference & show both Premise→Hypothesis and Hypothesis→Premise attention.\"\"\"\n",
    "    # 1) Tokenize → ids → tensors\n",
    "    ids1 = tokenize_to_ids([text1], nlp, MAX_LEN, NR_UNK)\n",
    "    ids2 = tokenize_to_ids([text2], nlp, MAX_LEN, NR_UNK)\n",
    "    x1 = torch.tensor(ids1, device=device)\n",
    "    x2 = torch.tensor(ids2, device=device)\n",
    "    l1 = (x1 != 0).sum(1)\n",
    "    l2 = (x2 != 0).sum(1)\n",
    "\n",
    "    # 2) Forward pass w/ attention\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, (att_p2h, att_h2p) = model(x1, l1, x2, l2, return_attention=True)\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    pred = probs.argmax()\n",
    "    print(f\"Prediction: {id2label[pred]}\")\n",
    "    print(\"Scores: \" + \", \".join(f\"{id2label[i]}={probs[i]:.3f}\" for i in range(len(probs))))\n",
    "\n",
    "    # 3) Extract tokens\n",
    "    toks1 = [t.text for t in list(nlp(text1))[:l1.item()]]\n",
    "    toks2 = [t.text for t in list(nlp(text2))[:l2.item()]]\n",
    "\n",
    "    # 4) Plot both attentions side‑by‑side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Premise → Hypothesis\n",
    "    A1 = att_p2h[0, :len(toks1), :len(toks2)].cpu().numpy()\n",
    "    sns.heatmap(\n",
    "        A1,\n",
    "        ax=axes[0],\n",
    "        xticklabels=toks2, yticklabels=toks1,\n",
    "        cmap=\"viridis\", cbar_kws={\"label\": \"attention\"},\n",
    "        fmt=\".2f\"\n",
    "    )\n",
    "    axes[0].set_title(\"Premise → Hypothesis\")\n",
    "    axes[0].set_xlabel(\"Hypothesis tokens\")\n",
    "    axes[0].set_ylabel(\"Premise tokens\")\n",
    "    axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[0].tick_params(axis=\"y\", rotation=0)\n",
    "\n",
    "    # Hypothesis → Premise\n",
    "    A2 = att_h2p[0, :len(toks2), :len(toks1)].cpu().numpy()\n",
    "    sns.heatmap(\n",
    "        A2,\n",
    "        ax=axes[1],\n",
    "        xticklabels=toks1, yticklabels=toks2,\n",
    "        cmap=\"magma\", cbar_kws={\"label\": \"attention\"},\n",
    "        fmt=\".2f\"\n",
    "    )\n",
    "    axes[1].set_title(\"Hypothesis → Premise\")\n",
    "    axes[1].set_xlabel(\"Premise tokens\")\n",
    "    axes[1].set_ylabel(\"Hypothesis tokens\")\n",
    "    axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "    axes[1].tick_params(axis=\"y\", rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"data/esim_nli_model.pt\", map_location=device, weights_only=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "premise   = \"A man inspects the uniform of a figure in some East Asian country.\"\n",
    "hypothesis= \"The man is sleeping.\"\n",
    "infer_and_plot(model, premise, hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad055e0",
   "metadata": {},
   "source": [
    "# BERT Model NLI Inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contradiction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
